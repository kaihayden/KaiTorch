{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d366922",
   "metadata": {},
   "source": [
    "# [Notebook 05] Loss Functions\n",
    "\n",
    "__Last notebook we trained a neural net on 5 samples by writing the following training loop__\n",
    "\n",
    "1. __Input Samples__ \n",
    "     - Feed input inputs into the model and perform a forward pass to get the predicted values.\n",
    "\n",
    "2. __Calculate Loss__ \n",
    "     - We'll use a simple maximum-margin hinge loss for binary classification. This is squared error where the labels are `-1 or 1`.\n",
    "\n",
    "3. __Zero Gradients__\n",
    "     - Manually set the gradients of all parameters to zero. If not done, the gradients for each Scalar value will accumulate during model training, which may lead to undesired behaviour. The PyTorch equivalent is `zero_grad()`.\n",
    "\n",
    "4. __Reverse-mode Autodiff__\n",
    "     - Begin backpropogation by performing a backward pass to calculate the gradients of every parameter (weight & bias) in the model.\n",
    "\n",
    "5. __Gradient Descent__\n",
    "     - Complete backpropogation by updating the parameter values of the model, each parameter will be adjusted by a `learning_rate` (set to 0.1) multiplied by the negative of the gradient. We'll go deeper into gradient descent in notebook 6.\n",
    "\n",
    "__In step 2, I kind of just threw a random loss function at you__, so in this notebook we'll be diving deeper into this step and building more functionality around our `Sequential` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8189ea7a",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "A loss function is a function that takes the __predicted output__ of our neural network and the __actual values__ from our training data as inputs, and outputs a __measure of difference between predicted and actual values__, called the __loss value__.\n",
    "\n",
    "This is useful when it comes to neural networks because we want to get our predicted outputs to match the actual values closely, or in other words, __minimize the loss value__. We can achieve this during our training using gradient descent.\n",
    "\n",
    "Since we can decompose functions into basic operations, it means loss functions are differentiable, and by calling `backward()` on the loss value, we can calculate the gradients of the whole network relative to to the loss value through backpropogation, and train the network by updating the weights.\n",
    "\n",
    "In this notebook we'll implement the 3 most common loss functions:\n",
    "\n",
    "- Mean Squared Error (Regression)\n",
    "- Binary Cross Entropy (Binary Classification)\n",
    "- Categorical Cross Entropy (Multi-class Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05403c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaitorch/losses.py\n",
    "\n",
    "from kaitorch.utils import wrap\n",
    "from kaitorch.core import Scalar\n",
    "\n",
    "__all__ = ['mse', 'binary_crossentropy', 'categorical_crossentropy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a2d7f",
   "metadata": {},
   "source": [
    "# Mean Squared Error\n",
    "\n",
    "Mean Squared Error (MSE) is often used for regression problems, where we are trying to predict a continuous and unbounded output value. It measures the average of the squared differences between the predicted output and the actual value for a given set of training data.\n",
    "\n",
    "__Breaking down MSE into its constituent terms__ (in reverse)\n",
    "\n",
    "- __Error__ - the error is the difference between the predicted output and the actual value\n",
    "\n",
    "- __Squared__ - squaring the error penalizes larger differences more heavily, making the model more likely to correct large errors during training.\n",
    "\n",
    "- __Mean__ - taking the mean of the squared error across multiple samples gives us a better estimate of the overall model prediction error (as opposed to using only the error of a single sample), reducing the variance of our model training.\n",
    "\n",
    "__MSE Formula__\n",
    "$$ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "- $N$ is the number of samples\n",
    "- $y_i$ is the actual value (continuous unbounded)\n",
    "- $\\hat{y}_i$ is the predicted value (continuous unbounded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3434d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaitorch/losses.py\n",
    "\n",
    "def mse():\n",
    "    return MeanSquaredError()\n",
    "\n",
    "class MeanSquaredError:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, ys: list, y_preds: list):\n",
    "\n",
    "        ys, y_preds = wrap(ys), wrap(y_preds)\n",
    "\n",
    "        # 1/N\n",
    "        pred_length = len(ys)\n",
    "        \n",
    "        # Summation Term\n",
    "        squared_error = sum(\n",
    "            (y - y_pred)**2 for y, y_pred in zip(ys, y_preds))\n",
    "        \n",
    "        # Mean Squared Error\n",
    "        mean_squared_error = squared_error/pred_length\n",
    "\n",
    "        return mean_squared_error\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'MeanSquaredError()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcf998ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When y=0, y_pred=0  , MSE loss is 0.0\n",
      "When y=0, y_pred=0.1, MSE loss is 0.01\n",
      "When y=0, y_pred=1  , MSE loss is 1.0\n",
      "When y=0, y_pred=10 , MSE loss is 100.0\n"
     ]
    }
   ],
   "source": [
    "# applying to individual samples\n",
    "MSE = mse()\n",
    "\n",
    "ys = [0, 0, 0, 0]\n",
    "y_preds = [0, 0.1, 1, 10]\n",
    "\n",
    "for y, y_pred in zip(ys, y_preds):\n",
    "    MSE_loss = MSE(y, y_pred)\n",
    "    print(f'When y={y}, y_pred={y_pred:<3}, MSE loss is {MSE_loss:.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8bbd3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When y=[0, 0, 0, 0], y_pred=[0, 0.1, 1, 10], MSE loss is 25.253\n"
     ]
    }
   ],
   "source": [
    "# applying to a list of samples\n",
    "MSE = mse()\n",
    "\n",
    "ys = [[0, 0, 0, 0]]\n",
    "y_preds = [[0, 0.1, 1, 10]]\n",
    "\n",
    "for y, y_pred in zip(ys, y_preds):\n",
    "    MSE_loss = MSE(y, y_pred)\n",
    "    print(f'When y={y}, y_pred={y_pred}, MSE loss is {MSE_loss:.5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4270b8b3",
   "metadata": {},
   "source": [
    "# Binary Cross Entropy\n",
    "\n",
    "Binary Cross Entropy (BCE) is often used in binary classification, problems where we are trying to determine if a sample belongs to the positive (1) or negative (0) class. BCE is a measure of __how well a model is able to distinguish between the positive and negative class.__\n",
    "\n",
    "\n",
    "__BCE Formula__\n",
    "$$ \\text{BCE} = -\\frac{1}{N} \\sum_{i=1}^N [y_i \\cdot \\ln \\hat{y}_i + (1-y_i) \\cdot\\ln (1-\\hat{y}_i)] $$\n",
    "\n",
    "- $N$ is the number of samples\n",
    "- $y_i$ is the actual class (0 or 1)\n",
    "- $\\hat{y}_i$ is the predicted class (continuous between 0 and 1)\n",
    "\n",
    "### Positive vs Negative Class\n",
    "\n",
    "An intersting behaviour of BCE is how either the left or right hand term of the loss function is inactive depending on the label of the data point.\n",
    "\n",
    " - __Left hand term:__ $y_i \\cdot \\ln \\hat{y}_i$ \n",
    " - __Right hand term:__ $(1-y_i) \\cdot \\ln (1-\\hat{y}_i)$\n",
    "\n",
    "\n",
    "When the actual class is positive ($y_i=1$):\n",
    " - $y_i$ = 1, and the left hand term is active\n",
    " - $1 - y_i = 0$ and the right hand term is inactive.\n",
    " - The loss function for the data point becomes $\\text{BCE} = \\ln \\hat{y}_i$\n",
    "\n",
    "When the actual class is negative ($y_i=0$):\n",
    " - $y_i = 0$, and the left hand term is inactive\n",
    " - $1 - y_i = 1$, and the right hand term is active.\n",
    " - The loss function for the data point becomes $\\text{BCE} = \\ln (1-\\hat{y}_i)$\n",
    "\n",
    "When evaluating any given data point, only half of this loss function is \"active\" (the other is multiplied by 0). So, the loss function becomes simply the __negative logarithm of the predicted probability of the true class__.\n",
    "\n",
    "When evaluating on a list of data points, simply average the losses of each point.\n",
    "\n",
    "### Why Negative Logarithm?\n",
    "\n",
    "Negative logarithm is used because it has the following property:\n",
    "- When $x=1, -\\ln(x)=0$, so when we predict the correct class, the error term is 0\n",
    "- When $x=0, -\\ln(x)=\\infty$, so when we predict the incorrect class, the error term grows increasingly large\n",
    "\n",
    "By trying to minimizing the Binary Crossentropy, the model is encouraged to predict probabilities that are closer to the true class, and infinitely discouraged from predicting the opposite class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0086915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaitorch/losses.py\n",
    "\n",
    "def binary_crossentropy():\n",
    "    return BinaryCrossentropy()\n",
    "\n",
    "class BinaryCrossentropy:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, ys, y_preds):\n",
    "\n",
    "        loss = 0.0\n",
    "        ys, y_preds = wrap(ys), wrap(y_preds)\n",
    "\n",
    "        # 1/N\n",
    "        pred_length = len(ys)\n",
    "\n",
    "        # Summation term - could've done this more concisely but wanted to make the logic clear\n",
    "        for y, y_pred in zip(ys, y_preds):\n",
    "\n",
    "            # active left term\n",
    "            if y == 1:\n",
    "                loss += -(y_pred).log()\n",
    "\n",
    "            # active right term\n",
    "            elif y == 0:\n",
    "                loss += -(1 - y_pred).log()\n",
    "\n",
    "        # Binary Cross Entropy\n",
    "        binary_crossentropy_loss = loss / pred_length\n",
    "\n",
    "        return binary_crossentropy_loss\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'BinaryCrossentropy()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76c008ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When y=1, y_pred=0.99, BCE loss is 0.01005\n",
      "When y=1, y_pred=0.01, BCE loss is 4.6052\n",
      "When y=0, y_pred=0.99, BCE loss is 4.6052\n",
      "When y=0, y_pred=0.01, BCE loss is 0.01005\n"
     ]
    }
   ],
   "source": [
    "# applying to individual samples\n",
    "bce = BinaryCrossentropy()\n",
    "\n",
    "ys = [1, 1, 0, 0]\n",
    "y_preds = [0.99, 0.01, 0.99, 0.01]\n",
    "\n",
    "for y, y_pred in zip(ys, y_preds):\n",
    "    bce_loss = bce(y, Scalar(y_pred))\n",
    "    print(f'When y={y}, y_pred={y_pred}, BCE loss is {bce_loss.data:.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94a1a5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When y=[1, 1, 0, 0], y_pred=[0.99, 0.01, 0.99, 0.01], BCE loss is 2.3076\n"
     ]
    }
   ],
   "source": [
    "# applying to a list of samples\n",
    "bce = BinaryCrossentropy()\n",
    "\n",
    "ys = [[1, 1, 0, 0]]\n",
    "y_preds = [[0.99, 0.01, 0.99, 0.01]]\n",
    "\n",
    "for y, y_pred in zip(ys, y_preds):\n",
    "    bce_loss = bce(y, [Scalar(y_p) for y_p in y_pred])\n",
    "    print(f'When y={y}, y_pred={y_pred}, BCE loss is {bce_loss.data:.5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4058e165",
   "metadata": {},
   "source": [
    "### Categorical Cross Entropy\n",
    "\n",
    "Categorical Cross Entropy (CCE) is often used for multi-class classification, problems where we are trying to determine which class a sample belongs to. CCE is a measure of __how well a model is able to distinguish the correct class__.\n",
    "\n",
    "CCE is the generalized version of BCE - instead of determining whether or not a sample belongs to a single class, it determines which, out of multiple classes, a sample belongs to.\n",
    "\n",
    "Class membership (or otherwise) is represented using __one-hot encodings__. For example, if we have 3 classes and a sample belongs to the second class, we represent this using the __one-hot vector__ `[0, 1, 0]`.\n",
    "\n",
    "__CCE Formula__\n",
    "$$ \\text{CCE} = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{j}^C [y_{ij} * \\ln \\hat{y}_{ij} + (1-y_{ij}) * \\ln (1-\\hat{y}_{ij})] $$\n",
    "\n",
    "- $N$ is the number of samples\n",
    "- $C$ is the number of classes\n",
    "- $y_i$ is the OHE vector of the actual class (list of 0s and 1s)\n",
    "- $y_{ij}$ is whether or not the actual class is `j` (list of 0s and 1s)\n",
    "- $\\hat{y}_i$ is the vector of the predcted classes (list of values between 0 and 1)\n",
    "- $\\hat{y}_{ij}$ is the predicted probability of class `j`\n",
    "\n",
    "In plain terms, this is how BCE generalizes to CCE:\n",
    "\n",
    "- With BCE, the question is whether or not a sample belongs to a class.\n",
    "- With CCE, the question is whether or not a sample belongs to class A, whether or not it belongs to class B, ... etc.\n",
    "\n",
    "Categorical cross entropy is essentially the sum of BCEs for every class in our problem, and as you'll see in the code below, the logic is essentially the same, with the introduction of a loop to iterate through the additional classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aa2f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaitorch/losses.py\n",
    "\n",
    "def categorical_crossentropy():\n",
    "    return CategoricalCrossentropy()\n",
    "\n",
    "class CategoricalCrossentropy:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, ys, y_preds):\n",
    "\n",
    "        loss = 0.0\n",
    "        if isinstance(ys[0], (int, float, Scalar)):\n",
    "            ys, y_preds = [ys], [y_preds]\n",
    "\n",
    "        # 1/N\n",
    "        pred_length = len(ys)\n",
    "\n",
    "        # Outer summation term - again, this could've been more concise but wanted to make the logic clear\n",
    "        for y_ohe, y_pred_ohe in zip(ys, y_preds):\n",
    "\n",
    "            # Inner summation term\n",
    "            for y, y_pred in zip(y_ohe, y_pred_ohe):\n",
    "                \n",
    "                # if j is the actual class\n",
    "                if y == 1:\n",
    "                    loss += -(y_pred).log()\n",
    "                    \n",
    "                # if j is not the actual class\n",
    "                elif y == 0:\n",
    "                    loss += -(1 - y_pred).log()\n",
    "\n",
    "        # Categorical Cross Entropy\n",
    "        categorical_crossentropy_loss = loss / pred_length\n",
    "\n",
    "        return categorical_crossentropy_loss\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'CategoricalCrossEntropy()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11d24b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When y=[1, 0, 0], y_pred=[0.99, 0.01, 0.01], CCE loss is 0.030151\n",
      "When y=[1, 0, 0], y_pred=[0.99, 0.01, 0.99], CCE loss is 4.6253\n",
      "When y=[1, 0, 0], y_pred=[0.01, 0.01, 0.99], CCE loss is 9.2204\n",
      "When y=[1, 0, 0], y_pred=[0.01, 0.99, 0.99], CCE loss is 13.816\n"
     ]
    }
   ],
   "source": [
    "# applying to individual samples\n",
    "cce = CategoricalCrossentropy()\n",
    "\n",
    "ys = [\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0]\n",
    "]\n",
    "\n",
    "y_preds = [\n",
    "    [0.99, 0.01, 0.01],\n",
    "    [0.99, 0.01, 0.99],\n",
    "    [0.01, 0.01, 0.99],\n",
    "    [0.01, 0.99, 0.99]\n",
    "]\n",
    "\n",
    "for y, y_pred in zip(ys, y_preds):\n",
    "    cce_loss = cce(y, [Scalar(y_p) for y_p in y_pred])\n",
    "    print(f'When y={y}, y_pred={y_pred}, CCE loss is {cce_loss.data:.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57376bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When y=[[1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0]], y_pred=[[0.99, 0.01, 0.01], [0.99, 0.01, 0.99], [0.01, 0.01, 0.99], [0.01, 0.99, 0.99]], CCE loss is 6.9228\n"
     ]
    }
   ],
   "source": [
    "# applying to a list of samples\n",
    "cce = CategoricalCrossentropy()\n",
    "\n",
    "ys = [[\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0]\n",
    "]]\n",
    "\n",
    "y_preds = [[\n",
    "    [0.99, 0.01, 0.01],\n",
    "    [0.99, 0.01, 0.99],\n",
    "    [0.01, 0.01, 0.99],\n",
    "    [0.01, 0.99, 0.99]\n",
    "]]\n",
    "\n",
    "for y, y_pred in zip(ys, y_preds):\n",
    "    cce_loss = cce(y, [[Scalar(y_p) for y_p in y_row] for y_row in y_pred])\n",
    "    print(f'When y={y}, y_pred={y_pred}, CCE loss is {cce_loss.data:.5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8c3860",
   "metadata": {},
   "source": [
    "Just for fun, if we wanted to (which we don't), we could plug BCE into our CCE :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70931f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __call__(self, ys, y_preds):\n",
    "    \n",
    "    print(\"Using BCE\") # just to prove we're using this __call__\n",
    "\n",
    "    if isinstance(ys[0], (int, float, Scalar)):\n",
    "        ys, y_preds = [ys], [y_preds]\n",
    "\n",
    "    # 1/N\n",
    "    pred_length = len(ys)\n",
    "\n",
    "    # Outer summation term\n",
    "    for y_ohe, y_pred_ohe in zip(ys, y_preds):\n",
    "\n",
    "        # need to multiple by 3 for each class\n",
    "        categorical_crossentropy_loss = 3 * BinaryCrossentropy()(y_ohe, y_pred_ohe) \n",
    "        #                                      ^^ BCE lol ^^ \n",
    "\n",
    "    # Categorical Cross Entropy\n",
    "    categorical_crossentropy_loss = categorical_crossentropy_loss / pred_length\n",
    "\n",
    "    return categorical_crossentropy_loss\n",
    "\n",
    "CategoricalCrossentropy.__call__ = __call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b8be87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BCE\n",
      "When y=[1, 0, 0], y_pred=[0.99, 0.01, 0.01], CCE loss is 0.030151\n",
      "Using BCE\n",
      "When y=[1, 0, 0], y_pred=[0.99, 0.01, 0.99], CCE loss is 4.6253\n",
      "Using BCE\n",
      "When y=[1, 0, 0], y_pred=[0.01, 0.01, 0.99], CCE loss is 9.2204\n",
      "Using BCE\n",
      "When y=[1, 0, 0], y_pred=[0.01, 0.99, 0.99], CCE loss is 13.816\n"
     ]
    }
   ],
   "source": [
    "# Verify we get the same result\n",
    "\n",
    "cce = CategoricalCrossentropy()\n",
    "\n",
    "ys = [\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0]\n",
    "]\n",
    "\n",
    "y_preds = [\n",
    "    [0.99, 0.01, 0.01],\n",
    "    [0.99, 0.01, 0.99],\n",
    "    [0.01, 0.01, 0.99],\n",
    "    [0.01, 0.99, 0.99]\n",
    "]\n",
    "\n",
    "for y, y_pred in zip(ys, y_preds):\n",
    "    cce_loss = cce(y, [Scalar(y_p) for y_p in y_pred])\n",
    "    print(f'When y={y}, y_pred={y_pred}, CCE loss is {cce_loss.data:.5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c7f09",
   "metadata": {},
   "source": [
    "### Those are our 3 loss functions in KaiTorch!\n",
    "\n",
    "Now that we have proper loss functions to work with, let's continue building functionality around our `Sequential` class.\n",
    "\n",
    "This is where we left off last notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5a1db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaitorch.core import Module\n",
    "from kaitorch.layers import Dense\n",
    "from kaitorch.utils import unwrap\n",
    "\n",
    "class Sequential(Module):\n",
    "\n",
    "    def __init__(self, layers=None):\n",
    "        self.built = False\n",
    "\n",
    "        self.layers = layers if layers else []\n",
    "        self.layer_sizes = [\n",
    "            layer.nouts for layer in self.layers\n",
    "            ] if self.layers else []\n",
    "    \n",
    "    def __repr__(self):\n",
    "        print([layer.parameters() for layer in self.layers])\n",
    "        return '\\n'.join(str(layer) for layer in self.layers)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return unwrap(x)\n",
    "        \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        self.layer_sizes.append(layer.nouts)\n",
    "    \n",
    "    def build(self, input_size):\n",
    "\n",
    "        if self.built:\n",
    "            return\n",
    "\n",
    "        self.layer_sizes.insert(0, input_size)\n",
    "\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            layer.__build__(self.layer_sizes[idx])\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def summary(self):\n",
    "        print('_' * 115)\n",
    "        print('Layer (params)' + ' '*59 + 'Output Shape' + ' '*5 + 'Params = Weights + Biases')\n",
    "        print('=' * 115)\n",
    "        for layer_num, layer in enumerate(self.layers):\n",
    "            l_name = layer.__repr__()\n",
    "            l_output = f'(None, {layer.nouts})'\n",
    "            l_params = len(layer.parameters())\n",
    "            l_w = l_params - layer.nouts if l_params > 0 else 0\n",
    "            l_b = layer.nouts if l_params > 0 else 0\n",
    "\n",
    "            print(f'{l_name:<73}{l_output:<17}{l_params:<9}{l_w:<10}{l_b:<6}')\n",
    "            if layer_num != (len(self.layers) - 1):\n",
    "                print('_' * 115)\n",
    "        print('=' * 115)\n",
    "        print(f'Total Params: {sum([len(layer.parameters()) for layer in self.layers])}')\n",
    "        print('_' * 115)\n",
    "\n",
    "    def plot(self, filename=None):\n",
    "\n",
    "        if not self.built:\n",
    "            raise Exception('[Model Not Built] - Use Sequential.build(input_size) to build model')\n",
    "        empty_input = self.__call__([0]*self.layer_sizes[0])\n",
    "        return plot_model(empty_input, filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a48513",
   "metadata": {},
   "source": [
    "### Module Class\n",
    "\n",
    "First, let's define the `Module` class that `Sequential` inherits that I introduced last notebook.\n",
    "\n",
    "Every layer and model will inherit this class that contains the `zero_grad` method (step 3 of our training loop) and a `parameter` method as a reminder that this method should be implemented for every `Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d86c81c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaitorch/core.py\n",
    "\n",
    "class Module:\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0.0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debfae48",
   "metadata": {},
   "source": [
    "### Sequential Call\n",
    "\n",
    "So far, we've only visited the training phase, but once a model is trained, we want to be able to run the model without updating its weights. Let's introduce a new parameter `train` when we call our model. It's currently unused in but we'll need it in Notebook 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed771c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __call__(self, x, train):\n",
    "    for layer in self.layers:\n",
    "        x = layer(x)\n",
    "    return unwrap(x)\n",
    "\n",
    "Sequential.__call__ = __call__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd73ae5",
   "metadata": {},
   "source": [
    "### Compiling our Sequential Model\n",
    "\n",
    "Let's incorporate our work on loss functions into `Sequential`. In our pursuit of being Keras-esque , we want to compile our model with these arguments.\n",
    "```python\n",
    "# Keras af\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "```\n",
    "\n",
    "In this notebook, we'll add a `compiled` attribute to `Sequential` and implement a `compile(loss=___)` method that sets the loss function of the model. `optimizer` will come in the next notebook :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87535c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaitorch/models.py\n",
    "\n",
    "def __init__(self, layers=None):\n",
    "    self.built = False\n",
    "    self.compiled = False\n",
    "\n",
    "    self.layers = layers if layers else []\n",
    "    self.layer_sizes = [layer.nouts for layer in self.layers] if self.layers else []\n",
    "    \n",
    "Sequential.__init__ = __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bc01d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaitorch.losses\n",
    "\n",
    "def compile(self, loss):\n",
    "\n",
    "    # kaitorch/models.py\n",
    "    def set_loss(loss):\n",
    "        if isinstance(loss, str):\n",
    "            if loss in kaitorch.losses.__all__:\n",
    "                self.loss = getattr(kaitorch.losses, loss)()\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    f'[Undefined Loss Function] - Loss Function \"{loss}\" has not been implemented'\n",
    "                )\n",
    "        else:\n",
    "            self.loss = loss\n",
    "            \n",
    "    if not self.compiled:\n",
    "        if loss:\n",
    "            set_loss(loss)\n",
    "            self.compiled = True\n",
    "        else:\n",
    "            raise Exception(\n",
    "                '[Unable to Compile] - Optimizer and Loss Function must be specified'\n",
    "            )\n",
    "            \n",
    "Sequential.compile = compile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7519429f",
   "metadata": {},
   "source": [
    "### Run\n",
    "\n",
    "Let's write each iteration of the training loop into a method `run`, and separate the functions that are only needed for training into an if statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2336f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def run(self, x, y=None, epoch=1, epochs=1, train=False):\n",
    "\n",
    "    postfix_type = 'Train' if train is True else ''\n",
    "\n",
    "    # Progress bar looping through all inputs\n",
    "    tqdm_x = tqdm(\n",
    "        x,\n",
    "        ncols=160,\n",
    "        desc=f\"Epoch {epoch:>3}/{epochs}\", \n",
    "        postfix='',\n",
    "        bar_format='{l_bar}{bar:40}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}{postfix}]'\n",
    "    )\n",
    "\n",
    "    # List to store model predictions\n",
    "    y_pred = []\n",
    "\n",
    "    # For every input\n",
    "    for x_record in tqdm_x:\n",
    "        \n",
    "        # Replacing [model(x) for x in xs]\n",
    "        y_pred.append(self.__call__(x_record, train=train))\n",
    "\n",
    "        # If an true y value is provided, calculate the loss value and display in progress bar\n",
    "        if y:\n",
    "            run_loss = self.loss(y[:len(y_pred)], y_pred)\n",
    "            tqdm_x.set_postfix_str(f\"{postfix_type} Loss: {run_loss.data:.4f}\")\n",
    "\n",
    "        # Else, loss value is None, and do not display in progress bar\n",
    "        else:\n",
    "            run_loss = None\n",
    "            tqdm_x.set_postfix_str(f\"{postfix_type}\")\n",
    "\n",
    "    # If this is a training run, \n",
    "    if train:\n",
    "        self.zero_grad() # using zero_grad() inherited from Module\n",
    "        run_loss.backward()\n",
    "        self.step() # we haven't defined this yet\n",
    "\n",
    "    return y_pred, run_loss\n",
    "\n",
    "Sequential.run = run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb1ab86",
   "metadata": {},
   "source": [
    "### Weight Update\n",
    "\n",
    "We haven't defined the `step()` method yet, as it has a dependency on whcih optimizer we choose to use, but for now we'll use the same simple gradient descent algorithm we used last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2648320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self, **optimizer_params):\n",
    "\n",
    "    for p in self.parameters():\n",
    "        p.data += 0.1 * -1 * p.grad\n",
    "        \n",
    "Sequential.step = step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38270ce",
   "metadata": {},
   "source": [
    "# Let's take it for a spin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbc094ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________________________________________________________________________________________\n",
      "Layer (params)                                                           Output Shape     Params = Weights + Biases\n",
      "===================================================================================================================\n",
      "Dense(units=3, activation=tanh, initializer=he_uniform)                  (None, 3)        12       9         3     \n",
      "___________________________________________________________________________________________________________________\n",
      "Dense(units=4, activation=tanh, initializer=he_uniform)                  (None, 4)        16       12        4     \n",
      "___________________________________________________________________________________________________________________\n",
      "Dense(units=4, activation=tanh, initializer=he_uniform)                  (None, 4)        20       16        4     \n",
      "___________________________________________________________________________________________________________________\n",
      "Dense(units=1, activation=tanh, initializer=glorot_uniform)              (None, 1)        5        4         1     \n",
      "===================================================================================================================\n",
      "Total Params: 53\n",
      "___________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(3, activation='tanh', initializer='he_uniform'))\n",
    "model.add(Dense(4, activation='tanh', initializer='he_uniform'))\n",
    "model.add(Dense(4, activation='tanh', initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='tanh'))\n",
    "\n",
    "model.compile(loss='mse')\n",
    "model.build(3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00ae8c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [[ 2.0,  3.0, -1.0],\n",
    "      [ 3.0, -1.0,  0.5],\n",
    "      [-0.5,  1.0,  1.0],\n",
    "      [ 1.0,  1.0, -1.0],\n",
    "      [ 2.5, -1.0, -1.0]]\n",
    "\n",
    "ys = [1.0, -1.0, -1.0, 1.0, -1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849c63d6",
   "metadata": {},
   "source": [
    "### Updated Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec093e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch   0/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 1.1228]\n",
      "Epoch   1/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.9291]\n",
      "Epoch   2/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.8039]\n",
      "Epoch   3/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.7046]\n",
      "Epoch   4/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.6111]\n",
      "Epoch   5/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.5191]\n",
      "Epoch   6/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.4313]\n",
      "Epoch   7/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.3532]\n",
      "Epoch   8/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.2879]\n",
      "Epoch   9/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.2357]\n",
      "Epoch  10/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.1946]\n",
      "Epoch  11/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.1623]\n",
      "Epoch  12/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.1367]\n",
      "Epoch  13/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.1163]\n",
      "Epoch  14/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0999]\n",
      "Epoch  15/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0865]\n",
      "Epoch  16/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0754]\n",
      "Epoch  17/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0663]\n",
      "Epoch  18/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0586]\n",
      "Epoch  19/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0521]\n",
      "Epoch  20/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0465]\n",
      "Epoch  21/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0418]\n",
      "Epoch  22/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0377]\n",
      "Epoch  23/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0342]\n",
      "Epoch  24/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0311]\n",
      "Epoch  25/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0283]\n",
      "Epoch  26/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0260]\n",
      "Epoch  27/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0238]\n",
      "Epoch  28/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0220]\n",
      "Epoch  29/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0203]\n",
      "Epoch  30/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0188]\n",
      "Epoch  31/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0174]\n",
      "Epoch  32/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0162]\n",
      "Epoch  33/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0151]\n",
      "Epoch  34/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0141]\n",
      "Epoch  35/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0132]\n",
      "Epoch  36/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0124]\n",
      "Epoch  37/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0116]\n",
      "Epoch  38/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0109]\n",
      "Epoch  39/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0103]\n",
      "Epoch  40/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0097]\n",
      "Epoch  41/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0092]\n",
      "Epoch  42/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0087]\n",
      "Epoch  43/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0082]\n",
      "Epoch  44/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0078]\n",
      "Epoch  45/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0074]\n",
      "Epoch  46/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0070]\n",
      "Epoch  47/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0067]\n",
      "Epoch  48/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0063]\n",
      "Epoch  49/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0060]\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    y_pred, run_loss = model.run(xs, ys, epoch, epochs, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55292800",
   "metadata": {},
   "source": [
    "__Pretty clean, right?__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
